{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fraud Detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulu0fL4si_iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report,f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27r90_ArzHWH",
        "colab_type": "text"
      },
      "source": [
        "Mounting the drive containing the creditcard data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybMrCqr9jwlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "870e81c5-9150-47a8-d7e2-a70705376c95"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jJY43P4zTQ0",
        "colab_type": "text"
      },
      "source": [
        "Loading in the creditcard data and showing a part of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbQP0KJ_jzSM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "8459619f-8933-4386-88ea-36734d554225"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/creditcard.csv\")\n",
        "df = df.sample(frac=1).reset_index()\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123173</td>\n",
              "      <td>76837.0</td>\n",
              "      <td>-1.532738</td>\n",
              "      <td>1.381636</td>\n",
              "      <td>1.972383</td>\n",
              "      <td>-1.627542</td>\n",
              "      <td>0.028538</td>\n",
              "      <td>-0.842331</td>\n",
              "      <td>1.331494</td>\n",
              "      <td>-0.806416</td>\n",
              "      <td>1.551211</td>\n",
              "      <td>1.719406</td>\n",
              "      <td>0.690998</td>\n",
              "      <td>-0.116192</td>\n",
              "      <td>-0.249479</td>\n",
              "      <td>-1.207764</td>\n",
              "      <td>0.924742</td>\n",
              "      <td>-0.295828</td>\n",
              "      <td>-0.666949</td>\n",
              "      <td>-1.164733</td>\n",
              "      <td>-1.378053</td>\n",
              "      <td>0.889070</td>\n",
              "      <td>-0.359956</td>\n",
              "      <td>0.020908</td>\n",
              "      <td>-0.060045</td>\n",
              "      <td>0.619002</td>\n",
              "      <td>-0.345443</td>\n",
              "      <td>0.612304</td>\n",
              "      <td>0.011339</td>\n",
              "      <td>-0.410334</td>\n",
              "      <td>7.68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>73724</td>\n",
              "      <td>55253.0</td>\n",
              "      <td>1.121078</td>\n",
              "      <td>0.136490</td>\n",
              "      <td>0.509031</td>\n",
              "      <td>1.327602</td>\n",
              "      <td>-0.247596</td>\n",
              "      <td>0.001118</td>\n",
              "      <td>-0.085944</td>\n",
              "      <td>0.158199</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.067681</td>\n",
              "      <td>1.227501</td>\n",
              "      <td>0.919692</td>\n",
              "      <td>-0.942222</td>\n",
              "      <td>0.410316</td>\n",
              "      <td>-0.735500</td>\n",
              "      <td>-0.555418</td>\n",
              "      <td>0.126657</td>\n",
              "      <td>-0.467081</td>\n",
              "      <td>-0.148328</td>\n",
              "      <td>-0.245544</td>\n",
              "      <td>-0.048987</td>\n",
              "      <td>0.042201</td>\n",
              "      <td>-0.035045</td>\n",
              "      <td>0.212967</td>\n",
              "      <td>0.584923</td>\n",
              "      <td>-0.339197</td>\n",
              "      <td>0.033312</td>\n",
              "      <td>0.005036</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>260425</td>\n",
              "      <td>159553.0</td>\n",
              "      <td>2.117533</td>\n",
              "      <td>0.053973</td>\n",
              "      <td>-2.214881</td>\n",
              "      <td>0.264774</td>\n",
              "      <td>0.621560</td>\n",
              "      <td>-0.718683</td>\n",
              "      <td>0.080700</td>\n",
              "      <td>-0.150646</td>\n",
              "      <td>0.900686</td>\n",
              "      <td>-0.426039</td>\n",
              "      <td>-1.342511</td>\n",
              "      <td>-1.177086</td>\n",
              "      <td>-1.640454</td>\n",
              "      <td>-0.432154</td>\n",
              "      <td>1.193336</td>\n",
              "      <td>0.532707</td>\n",
              "      <td>0.202044</td>\n",
              "      <td>1.000115</td>\n",
              "      <td>-0.126291</td>\n",
              "      <td>-0.264557</td>\n",
              "      <td>0.120386</td>\n",
              "      <td>0.399485</td>\n",
              "      <td>-0.056427</td>\n",
              "      <td>-0.082016</td>\n",
              "      <td>0.311290</td>\n",
              "      <td>-0.085282</td>\n",
              "      <td>-0.020171</td>\n",
              "      <td>-0.035374</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>129666</td>\n",
              "      <td>79177.0</td>\n",
              "      <td>-1.390803</td>\n",
              "      <td>1.557338</td>\n",
              "      <td>0.407697</td>\n",
              "      <td>-0.045973</td>\n",
              "      <td>-0.495813</td>\n",
              "      <td>-0.334196</td>\n",
              "      <td>-0.238542</td>\n",
              "      <td>1.123191</td>\n",
              "      <td>-0.656985</td>\n",
              "      <td>-0.502267</td>\n",
              "      <td>1.273355</td>\n",
              "      <td>0.518889</td>\n",
              "      <td>-0.809870</td>\n",
              "      <td>0.548186</td>\n",
              "      <td>0.493847</td>\n",
              "      <td>0.563310</td>\n",
              "      <td>0.416315</td>\n",
              "      <td>0.085635</td>\n",
              "      <td>-0.192590</td>\n",
              "      <td>-0.063017</td>\n",
              "      <td>-0.144256</td>\n",
              "      <td>-0.591522</td>\n",
              "      <td>0.114823</td>\n",
              "      <td>-0.061521</td>\n",
              "      <td>-0.200639</td>\n",
              "      <td>0.096879</td>\n",
              "      <td>0.127706</td>\n",
              "      <td>-0.006881</td>\n",
              "      <td>17.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>225009</td>\n",
              "      <td>144060.0</td>\n",
              "      <td>2.209576</td>\n",
              "      <td>-0.460521</td>\n",
              "      <td>-2.066690</td>\n",
              "      <td>-0.575268</td>\n",
              "      <td>0.196431</td>\n",
              "      <td>-0.957810</td>\n",
              "      <td>0.233858</td>\n",
              "      <td>-0.442743</td>\n",
              "      <td>-1.199704</td>\n",
              "      <td>1.111817</td>\n",
              "      <td>0.333758</td>\n",
              "      <td>0.834983</td>\n",
              "      <td>0.808542</td>\n",
              "      <td>0.466630</td>\n",
              "      <td>-1.220029</td>\n",
              "      <td>-1.845125</td>\n",
              "      <td>-0.276824</td>\n",
              "      <td>1.106933</td>\n",
              "      <td>-0.137624</td>\n",
              "      <td>-0.560353</td>\n",
              "      <td>-0.234469</td>\n",
              "      <td>-0.015364</td>\n",
              "      <td>-0.030071</td>\n",
              "      <td>-0.426780</td>\n",
              "      <td>0.294727</td>\n",
              "      <td>0.836056</td>\n",
              "      <td>-0.097516</td>\n",
              "      <td>-0.094097</td>\n",
              "      <td>10.94</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index      Time        V1        V2  ...       V27       V28  Amount  Class\n",
              "0  123173   76837.0 -1.532738  1.381636  ...  0.011339 -0.410334    7.68      0\n",
              "1   73724   55253.0  1.121078  0.136490  ...  0.033312  0.005036    1.00      0\n",
              "2  260425  159553.0  2.117533  0.053973  ... -0.020171 -0.035374    4.25      0\n",
              "3  129666   79177.0 -1.390803  1.557338  ...  0.127706 -0.006881   17.99      0\n",
              "4  225009  144060.0  2.209576 -0.460521  ... -0.097516 -0.094097   10.94      0\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccWFuUo5zpLk",
        "colab_type": "text"
      },
      "source": [
        "I  split up the data in two dataframes. One contains the frauds and the other non-frauds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svMJzsDKj9Xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d1f17f78-b139-40a8-bc80-15734b86c61e"
      },
      "source": [
        "frauds = df.query(\"Class == 1\")\n",
        "non_frauds = df.query(\"Class == 0\")\n",
        "print(len(frauds))\n",
        "print(len(non_frauds))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "492\n",
            "284315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqUYncHct136",
        "colab_type": "text"
      },
      "source": [
        "Plotting both frauds and non frauds as a histogram. Majority is non-frauds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02U9rFTSkF25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "cb59049c-a6a2-4ab0-9ea0-90d79c8b53f2"
      },
      "source": [
        "sns.distplot(frauds[\"Amount\"], color=\"blue\", label=\"fraud\")\n",
        "sns.distplot(non_frauds[\"Amount\"], color=\"orange\", label=\"non-fraud\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f81493cc828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0GS1BgIkiQA/j8WzRbUZVN3DQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YzwIx0SuOK3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now preparing x and y. Splitting into feature columns and label. The last column is used as label, and the rest as feature. I split the data into test an training data using train test split. Test is 40 % and training is 60 %. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n1Y_2N1kLoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = scale(np.array(df.iloc[:, :-1]))\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKQjxm3Pvb-U",
        "colab_type": "text"
      },
      "source": [
        "Now I train the model I want to use for prediciting future frauds. I chose logistic regsion as my model, which I train on my training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSh6VdIokNoK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "4270859e-0e60-4bd7-d9d4-8e915b6d6046"
      },
      "source": [
        "logistic = LogisticRegression(C=1e5)\n",
        "logistic.fit(X_train, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXIdJvIbv8YV",
        "colab_type": "text"
      },
      "source": [
        "Now that my model has been trained on my data I want to try predict future frauds with my test data. I print out the f score, which gives a measure on the test accuracty. A f1 score has the best value at 1 and worst at 0. Micro calculates globally by counting the total amount of true positives and false positives. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N7PeyJ3kPIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93b9a71a-22f7-47d0-e18f-57e4b80cfacd"
      },
      "source": [
        "y_pred = np.array(logistic.predict(X_test))\n",
        "score = f1_score(y_pred=y_pred,y_true=y_test,average='micro')\n",
        "\n",
        "score"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9992451041492938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ztXE2MsyrHD",
        "colab_type": "text"
      },
      "source": [
        "I use the classification report to conclude on my main classification metrics. I can see that precision for Non-fraud is 1.0, which means the model has predicted perfectly the positives that were positives. The non-frauds is a lower on 0.87 on precision. Reason might be overfitting, but then it might also have affected the non-frauds. Recall measures the share of predicted positives over the actual number of positives. Recall is a bit lower for the fraud. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeZtCJqnkSjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "8b438a34-0359-468f-c60d-28b2fd37bdeb"
      },
      "source": [
        "creport = classification_report(y_pred=y_pred,y_true=y_test,target_names=['Non-fraud','Fraud'])\n",
        "print(creport)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Non-fraud       1.00      1.00      1.00    113722\n",
            "       Fraud       0.87      0.68      0.76       201\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.93      0.84      0.88    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
